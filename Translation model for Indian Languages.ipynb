{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a45e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912238df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_LEN = 100\n",
    "DECODER_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = BATCH_SIZE*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7147b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_id</th>\n",
       "      <th>value</th>\n",
       "      <th>Source</th>\n",
       "      <th>Indic_Sentence</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Labse_Score</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>language_id</td>\n",
       "      <td>value</td>\n",
       "      <td>Source</td>\n",
       "      <td>Indic Sentence</td>\n",
       "      <td>English Sentence</td>\n",
       "      <td>Labse Score</td>\n",
       "      <td>bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>4</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>4</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>5</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>3</td>\n",
       "      <td>NPTEL</td>\n",
       "      <td>ક્ષેત્ર ઘટાડો.</td>\n",
       "      <td>Area reduction.</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>Definitely accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language_id  value     Source  \\\n",
       "0  language_id  value     Source   \n",
       "1     Gujarati      4  Wikipedia   \n",
       "2     Gujarati      4  Wikipedia   \n",
       "3     Gujarati      5  Wikipedia   \n",
       "4     Gujarati      3      NPTEL   \n",
       "\n",
       "                                      Indic_Sentence  \\\n",
       "0                                     Indic Sentence   \n",
       "1  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "2  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "3  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "4                                     ક્ષેત્ર ઘટાડો.   \n",
       "\n",
       "                                    English_Sentence         Labse_Score  \\\n",
       "0                                   English Sentence         Labse Score   \n",
       "1  Office is produced in several versions targete...  0.7818931340999999   \n",
       "2  Office is produced in several versions targete...  0.7818931340999999   \n",
       "3  Office is produced in several versions targete...  0.7818931340999999   \n",
       "4                                    Area reduction.              0.8527   \n",
       "\n",
       "              bucket  \n",
       "0             bucket  \n",
       "1  Marginally accept  \n",
       "2  Marginally accept  \n",
       "3  Marginally accept  \n",
       "4  Definitely accept  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# Passing the TSV file to\n",
    "# read_csv() function\n",
    "# with tab separator\n",
    "# This function will\n",
    "# read data from file\n",
    "interviews_df = pd.read_csv('C:/Users/gouth/Downloads/human_annotations.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "train_file_path = 'C:/Users/gouth/Downloads/human_annotations.tsv'\n",
    "\n",
    " \n",
    "with open(train_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "    \n",
    "data_split = [line.strip().split('\\t') for line in data]\n",
    "\n",
    "df = pd.DataFrame(data_split, columns=['language_id', 'value','Source','Indic_Sentence','English_Sentence','Labse_Score','bucket'])\n",
    "\n",
    "\n",
    "# printing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b0d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_id</th>\n",
       "      <th>value</th>\n",
       "      <th>Source</th>\n",
       "      <th>Indic_Sentence</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Labse_Score</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>4</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>4</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>5</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...</td>\n",
       "      <td>Office is produced in several versions targete...</td>\n",
       "      <td>0.7818931340999999</td>\n",
       "      <td>Marginally accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>3</td>\n",
       "      <td>NPTEL</td>\n",
       "      <td>ક્ષેત્ર ઘટાડો.</td>\n",
       "      <td>Area reduction.</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>Definitely accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>5</td>\n",
       "      <td>NPTEL</td>\n",
       "      <td>ક્ષેત્ર ઘટાડો.</td>\n",
       "      <td>Area reduction.</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>Definitely accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language_id value     Source  \\\n",
       "1    Gujarati     4  Wikipedia   \n",
       "2    Gujarati     4  Wikipedia   \n",
       "3    Gujarati     5  Wikipedia   \n",
       "4    Gujarati     3      NPTEL   \n",
       "5    Gujarati     5      NPTEL   \n",
       "\n",
       "                                      Indic_Sentence  \\\n",
       "1  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "2  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "3  અલગ અલગ વપરાશકર્તાઓ અને કમ્પ્યુટિંગ એન્વાયન્મે...   \n",
       "4                                     ક્ષેત્ર ઘટાડો.   \n",
       "5                                     ક્ષેત્ર ઘટાડો.   \n",
       "\n",
       "                                    English_Sentence         Labse_Score  \\\n",
       "1  Office is produced in several versions targete...  0.7818931340999999   \n",
       "2  Office is produced in several versions targete...  0.7818931340999999   \n",
       "3  Office is produced in several versions targete...  0.7818931340999999   \n",
       "4                                    Area reduction.              0.8527   \n",
       "5                                    Area reduction.              0.8527   \n",
       "\n",
       "              bucket  \n",
       "1  Marginally accept  \n",
       "2  Marginally accept  \n",
       "3  Marginally accept  \n",
       "4  Definitely accept  \n",
       "5  Definitely accept  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[1:]  # Removes the first row\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06069d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 41453\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8778839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_id</th>\n",
       "      <th>value</th>\n",
       "      <th>Source</th>\n",
       "      <th>Indic_Sentence</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Labse_Score</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>1</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>NPTEL</td>\n",
       "      <td>मान्य प्रश्न है हाँ, यह विरोधाभासी लग सकता है,...</td>\n",
       "      <td>And on the other hand I am saying that there i...</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language_id value     Source  \\\n",
       "4589       Hindi     2  Wikipedia   \n",
       "4590       Hindi     1  Wikipedia   \n",
       "4591       Hindi     3  Wikipedia   \n",
       "4592       Hindi     3  Wikipedia   \n",
       "4593       Hindi     3      NPTEL   \n",
       "\n",
       "                                         Indic_Sentence  \\\n",
       "4589  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4590  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4591  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4592  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4593  मान्य प्रश्न है हाँ, यह विरोधाभासी लग सकता है,...   \n",
       "\n",
       "                                       English_Sentence   Labse_Score  \\\n",
       "4589  It was considered the most important of the Ma...  0.7216691375   \n",
       "4590  It was considered the most important of the Ma...  0.7216691375   \n",
       "4591  It was considered the most important of the Ma...  0.7216691375   \n",
       "4592  It was considered the most important of the Ma...  0.7216691375   \n",
       "4593  And on the other hand I am saying that there i...        0.7154   \n",
       "\n",
       "                 bucket  \n",
       "4589  Marginally reject  \n",
       "4590  Marginally reject  \n",
       "4591  Marginally reject  \n",
       "4592  Marginally reject  \n",
       "4593  Marginally reject  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['language_id'] == 'Hindi']\n",
    "\n",
    "# Displaying the extracted rows where only 'Hindi' is present in 'language_id'\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8822c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'bucket' column:\n",
      "['Marginally accept' 'Definitely accept' 'Marginally reject' None]\n"
     ]
    }
   ],
   "source": [
    "unique_buckets = df['bucket'].unique()\n",
    "print(\"Unique values in 'bucket' column:\")\n",
    "print(unique_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8c5e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gouth\\AppData\\Local\\Temp\\ipykernel_28792\\2873094891.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.drop(['Source'],axis=1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language_id</th>\n",
       "      <th>value</th>\n",
       "      <th>Indic_Sentence</th>\n",
       "      <th>English_Sentence</th>\n",
       "      <th>Labse_Score</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>2</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>1</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...</td>\n",
       "      <td>It was considered the most important of the Ma...</td>\n",
       "      <td>0.7216691375</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>3</td>\n",
       "      <td>मान्य प्रश्न है हाँ, यह विरोधाभासी लग सकता है,...</td>\n",
       "      <td>And on the other hand I am saying that there i...</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>Marginally reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language_id value                                     Indic_Sentence  \\\n",
       "4589       Hindi     2  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4590       Hindi     1  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4591       Hindi     3  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4592       Hindi     3  इनके अलावा ये रियासतें थी - बड़ोदा रियासत ,ग्व...   \n",
       "4593       Hindi     3  मान्य प्रश्न है हाँ, यह विरोधाभासी लग सकता है,...   \n",
       "\n",
       "                                       English_Sentence   Labse_Score  \\\n",
       "4589  It was considered the most important of the Ma...  0.7216691375   \n",
       "4590  It was considered the most important of the Ma...  0.7216691375   \n",
       "4591  It was considered the most important of the Ma...  0.7216691375   \n",
       "4592  It was considered the most important of the Ma...  0.7216691375   \n",
       "4593  And on the other hand I am saying that there i...        0.7154   \n",
       "\n",
       "                 bucket  \n",
       "4589  Marginally reject  \n",
       "4590  Marginally reject  \n",
       "4591  Marginally reject  \n",
       "4592  Marginally reject  \n",
       "4593  Marginally reject  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(['Source'],axis=1,inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb4372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = train_df['English_Sentence']\n",
    "hind = train_df['Indic_Sentence']\n",
    "eng = eng.apply(lambda x: \"<SOS> \" + str(x) + \" <EOS>\")\n",
    "hind = hind.apply(lambda x: \"<SOS> \"+ x + \" <EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ac5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'\n",
    "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
    "hind_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
    "eng_tokenizer.fit_on_texts(eng)\n",
    "hind_tokenizer.fit_on_texts(hind)\n",
    "inputs = eng_tokenizer.texts_to_sequences(eng)\n",
    "targets = hind_tokenizer.texts_to_sequences(hind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c134d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3861 5776\n"
     ]
    }
   ],
   "source": [
    "ENCODER_VOCAB = len(eng_tokenizer.word_index) + 1\n",
    "DECODER_VOCAB = len(hind_tokenizer.word_index) + 1\n",
    "print(ENCODER_VOCAB, DECODER_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b738bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3f79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965541ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7001f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa6a9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8e16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6559b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd048322",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (train_df['English_Sentence'].str.len()>20) & (train_df['English_Sentence'].str.len()<200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2113ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef662235",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a6a1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "808858d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c5be95f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "227530a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cdfda7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c8fb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b03b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=ENCODER_VOCAB,\n",
    "    target_vocab_size=DECODER_VOCAB,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84cd4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09db5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "948a4739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4616"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39462868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.0987 Accuracy 0.1982\n",
      "Epoch 1 Loss 2.6124 Accuracy 0.2142\n",
      "Time taken for 1 epoch: 152.40794277191162 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6644 Accuracy 0.2144\n",
      "Epoch 2 Loss 2.2993 Accuracy 0.2316\n",
      "Time taken for 1 epoch: 154.853031873703 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.4823 Accuracy 0.2318\n",
      "Epoch 3 Loss 2.0045 Accuracy 0.2499\n",
      "Time taken for 1 epoch: 151.54523134231567 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.1762 Accuracy 0.2501\n",
      "Epoch 4 Loss 1.7227 Accuracy 0.2689\n",
      "Time taken for 1 epoch: 141.8621575832367 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.8524 Accuracy 0.2691\n",
      "Saving checkpoint for epoch 5 at checkpoints\\ckpt-5\n",
      "Epoch 5 Loss 1.4444 Accuracy 0.2883\n",
      "Time taken for 1 epoch: 138.06502652168274 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.6976 Accuracy 0.2886\n",
      "Epoch 6 Loss 1.2070 Accuracy 0.3078\n",
      "Time taken for 1 epoch: 155.81656789779663 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.4395 Accuracy 0.3080\n",
      "Epoch 7 Loss 0.9966 Accuracy 0.3270\n",
      "Time taken for 1 epoch: 148.44600296020508 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1382 Accuracy 0.3272\n",
      "Epoch 8 Loss 0.8189 Accuracy 0.3458\n",
      "Time taken for 1 epoch: 144.55074644088745 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0193 Accuracy 0.3460\n",
      "Epoch 9 Loss 0.6730 Accuracy 0.3640\n",
      "Time taken for 1 epoch: 136.35709476470947 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9285 Accuracy 0.3643\n",
      "Saving checkpoint for epoch 10 at checkpoints\\ckpt-6\n",
      "Epoch 10 Loss 0.5617 Accuracy 0.3816\n",
      "Time taken for 1 epoch: 146.2101595401764 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "   \n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "daa6cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text):\n",
    "    text = eng_tokenizer.texts_to_sequences([text])\n",
    "    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=ENCODER_LEN, \n",
    "                                                                   padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(text[0], 0)\n",
    "\n",
    "    decoder_input = [hind_tokenizer.word_index['<sos>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(DECODER_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == hind_tokenizer.word_index['<eos>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8637416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(eng_text):\n",
    "    hind_text = evaluate(text=eng_text)[0].numpy()\n",
    "    hind_text = np.expand_dims(hind_text[1:], 0)  \n",
    "    return hind_tokenizer.sequences_to_texts(hind_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7c72445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'यह कोई अच्छी बात नहीं है'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"This is not a good idea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b7ddef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'हम बात कर रहे हैं राजस्थान की।'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"We are talking about Rajasthan\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
